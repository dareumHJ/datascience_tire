# TabPFN Tire Quality Prediction - Refactored

íƒ€ì´ì–´ í’ˆì§ˆ ì˜ˆì¸¡ì„ ìœ„í•œ TabPFN ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ (ë¦¬íŒ©í† ë§ ë²„ì „)

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
project/
â”œâ”€â”€ config.yaml              # ëª¨ë“  ì„¤ì •ê°’ (í•˜ì´í¼íŒŒë¼ë¯¸í„°, ê²½ë¡œ ë“±)
â”œâ”€â”€ main.py                  # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ data_loader.py           # ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
â”œâ”€â”€ feature_engineering.py   # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ì••ë ¥, PCA, ì´ìƒì¹˜ íƒì§€ ë“±)
â”œâ”€â”€ feature_selection.py     # 3ë‹¨ê³„ í”¼ì²˜ ì„ íƒ (Model-free â†’ Stability â†’ Validation)
â”œâ”€â”€ model_training.py        # TabPFN ì•™ìƒë¸” í•™ìŠµ
â”œâ”€â”€ utils.py                 # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
â”œâ”€â”€ README.md               # ì´ íŒŒì¼
â”œâ”€â”€ train.csv               # í•™ìŠµ ë°ì´í„° (ID ì—†ìŒ, íƒ€ê²Ÿ: Class)
â”œâ”€â”€ test.csv                # í…ŒìŠ¤íŠ¸ ë°ì´í„° (ID ìˆìŒ)
â””â”€â”€ sample_submission.csv   # ì œì¶œ ì–‘ì‹
```

## ğŸ“Š ë°ì´í„° í˜•ì‹

### train.csv (720 samples, 799 columns)
- **íƒ€ê²Ÿ ì»¬ëŸ¼**: `Class` (ë¬¸ìì—´: "Good"=ì–‘í’ˆ, "NG"=ë¶ˆëŸ‰)
  - ìë™ìœ¼ë¡œ ìˆ«ìë¡œ ë³€í™˜ë¨: Goodâ†’0, NGâ†’1
  - ë¶„í¬: Good=613, NG=107
- **ID ì»¬ëŸ¼**: ì—†ìŒ (ìë™ ìƒì„±)
- **í”¼ì²˜**: Mass_Pilot, Width, Aspect, Inch, Plant, Proc_Param1-11, X1-X5, Y1-Y5, G1-G4, p0-p255, x0-x255, y0-y255

### test.csv (466 samples, 799 columns)
- **ID ì»¬ëŸ¼**: ID_0, ID_1, ... (ìˆìŒ)
- **í”¼ì²˜**: train.csvì™€ ë™ì¼ (Class ì œì™¸)

### sample_submission.csv
- **í˜•ì‹**: ID, probability, decision
- **ID**: ID_0_L, ID_0_R, ID_1_L, ID_1_R, ... (ì¢Œìš° íƒ€ì´ì–´)

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‹¤í–‰
```bash
python main.py
```

### 2. ì„¤ì • ë³€ê²½
`config.yaml` íŒŒì¼ì„ ìˆ˜ì •í•˜ì—¬ íŒŒë¼ë¯¸í„° ì¡°ì •:

```yaml
# ëª¨ë¸ ê°œìˆ˜ ë³€ê²½
model:
  n_models: 10  # ì•™ìƒë¸” ëª¨ë¸ ê°œìˆ˜

# í”¼ì²˜ ê°œìˆ˜ ë³€ê²½
features:
  n_features: 80  # ìµœì¢… ì„ íƒí•  í”¼ì²˜ ê°œìˆ˜

# ì„ íƒ ì „ëµ ë³€ê²½
selection:
  max_samples: 200  # ìµœëŒ€ ì„ íƒ ìƒ˜í”Œ ìˆ˜
  consensus_priority:  # ìš°ì„ ìˆœìœ„ ë³€ê²½ ê°€ëŠ¥
    - unanimous
    - strong
```

## ğŸ“Š ì£¼ìš” ê¸°ëŠ¥

### 1. Feature Engineering (`feature_engineering.py`)
- **ì••ë ¥ ê¸°ë°˜ í”¼ì²˜**: ë¹„ëŒ€ì¹­ì„±, ì§‘ì¤‘ë„, ê·¸ë˜ë””ì–¸íŠ¸
- **íƒ€ì´ì–´ í†µê³„ í”¼ì²˜**: í‰ê· , í‘œì¤€í¸ì°¨, ì™œë„, ì²¨ë„ ë“± 60+ í”¼ì²˜
- **PCA í”¼ì²˜**: ì••ë ¥ ë°ì´í„°ì˜ ì£¼ì„±ë¶„ ë¶„ì„
- **ì´ìƒì¹˜ íƒì§€**: Isolation Forest ê¸°ë°˜ ì´ìƒì¹˜ ìŠ¤ì½”ì–´

### 2. Feature Selection (`feature_selection.py`)
**3ë‹¨ê³„ ì„ íƒ ì „ëµ (Split Bias ìµœì†Œí™”):**

1. **Phase 1 - Model-Free Filtering**
   - Mutual Information
   - ANOVA F-test
   - Spearman Correlation
   - íˆ¬í‘œ ê¸°ë°˜ ìƒìœ„ 40ê°œ ì„ íƒ

2. **Phase 2 - Stability Selection**
   - 20ë²ˆì˜ ë‹¤ë¥¸ random split
   - TabPFN permutation importance
   - ì•ˆì •ì ìœ¼ë¡œ ì„ íƒë˜ëŠ” í”¼ì²˜ë§Œ ìœ ì§€

3. **Phase 3 - Final Validation**
   - 5-Fold Cross Validation
   - ìµœì  í”¼ì²˜ ê°œìˆ˜ ê²°ì •

### 3. Model Training (`model_training.py`)
- **ì•™ìƒë¸” ì „ëµ**: ì—¬ëŸ¬ random splitìœ¼ë¡œ í•™ìŠµí•œ TabPFN ëª¨ë¸ë“¤
- **Consensus Voting**: unanimous / strong / majority ë ˆë²¨
- **Best Model Selection**: Validation AUC ê¸°ì¤€

### 4. Selection Strategy
```python
# Consensus ë ˆë²¨ë³„ ì„ íƒ
unanimous:  100% ëª¨ë¸ì´ Good íŒì •
strong:     85-90% ëª¨ë¸ì´ Good íŒì •
majority:   60%+ ëª¨ë¸ì´ Good íŒì •
```

## âš™ï¸ ì„¤ì • íŒŒë¼ë¯¸í„° (config.yaml)

### ê²½ë¡œ ì„¤ì •
```yaml
paths:
  train_data: "train.csv"
  test_data: "test.csv"
  output_file: "submission_tabpfn_optimized.csv"
```

### ëª¨ë¸ íŒŒë¼ë¯¸í„°
```yaml
model:
  n_models: 10          # ì•™ìƒë¸” ëª¨ë¸ ê°œìˆ˜ (ì¶”ì²œ: 5-15)
  n_estimators: 1       # TabPFN estimators (ë³´í†µ 1ë¡œ ê³ ì •)
  model_version: "v12"  # v11 ë˜ëŠ” v12
  val_size: 0.2         # Validation ë¹„ìœ¨
```

### í”¼ì²˜ ì„¤ì •
```yaml
features:
  n_features: 80        # ìµœì¢… ì„ íƒ í”¼ì²˜ ê°œìˆ˜ (ì¶”ì²œ: 60-100)
  pca_components: 10    # PCA ì»´í¬ë„ŒíŠ¸ ê°œìˆ˜
  
  anomaly:
    contamination: 0.1   # Isolation Forest ì˜¤ì—¼ë¥ 
    n_estimators: 100    # Isolation Forest íŠ¸ë¦¬ ê°œìˆ˜
```

### ì„ íƒ ì „ëµ
```yaml
selection:
  max_samples: 200           # ìµœëŒ€ ì„ íƒ ìƒ˜í”Œ (200 ê³ ì •)
  max_probability: 1.0       # í™•ë¥  í•„í„° (1.0 = í•„í„° ì—†ìŒ)
  
  consensus_priority:        # ìš°ì„ ìˆœìœ„ (ìˆœì„œ ì¤‘ìš”!)
    - unanimous              # ë¨¼ì € unanimous ì„ íƒ
    - strong                 # ê·¸ ë‹¤ìŒ strong ì„ íƒ
```

## ğŸ”§ ì£¼ìš” ëª¨ë“ˆ ì„¤ëª…

### data_loader.py
- `load_data()`: CSV íŒŒì¼ ë¡œë”©
- `preprocess_data()`: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ì»¬ëŸ¼ ì •ë ¬

### feature_engineering.py
- `add_pressure_features()`: ê¸°ë³¸ ì••ë ¥ í”¼ì²˜
- `extract_tire_features()`: 60+ íƒ€ì´ì–´ ë¬¼ë¦¬ í”¼ì²˜
- `add_pca_features()`: PCA ë³€í™˜
- `add_anomaly_features()`: ì´ìƒì¹˜ íƒì§€ í”¼ì²˜
- `engineer_features()`: ì „ì²´ íŒŒì´í”„ë¼ì¸

### feature_selection.py
- `phase1_model_free_filtering()`: Model-free ì´ˆê¸° í•„í„°ë§
- `phase2_stability_selection()`: ì•ˆì •ì„± ê¸°ë°˜ ì„ íƒ
- `phase3_final_validation()`: CV ê¸°ë°˜ ìµœì¢… ê²€ì¦
- `split_bias_free_feature_selection()`: ì „ì²´ 3ë‹¨ê³„ ì‹¤í–‰

### model_training.py
- `find_optimal_threshold()`: Good Precision ìµœëŒ€í™” threshold
- `train_ensemble_tabpfn()`: ì•™ìƒë¸” í•™ìŠµ ë° íˆ¬í‘œ

### utils.py
- `load_config()`: YAML ì„¤ì • ë¡œë”©
- `create_submission()`: ì œì¶œ íŒŒì¼ ìƒì„±
- `print_selection_summary()`: ê²°ê³¼ ìš”ì•½ ì¶œë ¥

## ğŸ“ˆ ì‹¤í–‰ ê²°ê³¼

ì‹¤í–‰ ì‹œ ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ê°€ ì¶œë ¥ë©ë‹ˆë‹¤:

1. **ë°ì´í„° ë¡œë”©**: ìƒ˜í”Œ ìˆ˜, íƒ€ê²Ÿ ë¶„í¬
2. **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§**: ìƒì„±ëœ í”¼ì²˜ ìˆ˜
3. **í”¼ì²˜ ì„ íƒ**: 3ë‹¨ê³„ ê³¼ì • ë° ìµœì¢… ì„ íƒ í”¼ì²˜
4. **ëª¨ë¸ í•™ìŠµ**: ê° ëª¨ë¸ì˜ ì„±ëŠ¥ (AUC, Precision, Recall)
5. **Consensus ê²°ê³¼**: ê° ë ˆë²¨ë³„ ìƒ˜í”Œ ìˆ˜
6. **ìµœì¢… ì„ íƒ**: ì„ íƒëœ ìƒ˜í”Œ í†µê³„ ë° êµ¬ì„±

## ğŸ¯ ê¶Œì¥ ì‹¤í—˜ ì„¤ì •

### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
```yaml
model:
  n_models: 3
features:
  n_features: 50
```

### ê¸°ë³¸ ì„¤ì • (ì¶”ì²œ)
```yaml
model:
  n_models: 10
features:
  n_features: 80
```

### ê³ ì„±ëŠ¥ ì„¤ì •
```yaml
model:
  n_models: 15
features:
  n_features: 100
```

## ğŸ“ ì›ë³¸ ì½”ë“œì™€ì˜ ì°¨ì´ì 

### ê°œì„  ì‚¬í•­:
1. âœ… **ëª¨ë“ˆí™”**: ë‹¨ì¼ íŒŒì¼ â†’ 7ê°œ ëª¨ë“ˆë¡œ ë¶„ë¦¬
2. âœ… **ì„¤ì • ì™¸ë¶€í™”**: í•˜ë“œì½”ë”©ëœ ê°’ â†’ YAML ì„¤ì • íŒŒì¼
3. âœ… **ì¬ì‚¬ìš©ì„±**: í•¨ìˆ˜ë³„ ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥
4. âœ… **ìœ ì§€ë³´ìˆ˜**: ê° ëª¨ë“ˆì´ ëª…í™•í•œ ì—­í•  ë‹´ë‹¹
5. âœ… **í™•ì¥ì„±**: ìƒˆë¡œìš´ í”¼ì²˜/ëª¨ë¸ ì¶”ê°€ ìš©ì´

### ìœ ì§€ëœ ê¸°ëŠ¥:
- ëª¨ë“  í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¡œì§
- 3ë‹¨ê³„ í”¼ì²˜ ì„ íƒ ì „ëµ
- TabPFN ì•™ìƒë¸” ë°©ì‹
- Consensus ê¸°ë°˜ ì„ íƒ ì „ëµ

## ğŸ” ë””ë²„ê¹… ë° ì‹¤í—˜

### íŠ¹ì • ë‹¨ê³„ë§Œ ì‹¤í–‰í•˜ê¸°
```python
from feature_engineering import extract_tire_features
from utils import load_config

# í”¼ì²˜ë§Œ ì¶”ì¶œí•´ì„œ í™•ì¸
config = load_config('config.yaml')
features = extract_tire_features(train_df)
print(features.head())
```

### ì„¤ì •ê°’ í…ŒìŠ¤íŠ¸
```python
# ì—¬ëŸ¬ n_features ê°’ í…ŒìŠ¤íŠ¸
for n_feat in [50, 80, 100]:
    # config.yamlì˜ n_features ë³€ê²½ í›„
    # python main.py ì‹¤í–‰
```

## ğŸ“ ë¬¸ì˜ ë° ê°œì„ ì‚¬í•­

ì½”ë“œ ê°œì„  ì œì•ˆì´ë‚˜ ë²„ê·¸ ë¦¬í¬íŠ¸ëŠ” ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”!